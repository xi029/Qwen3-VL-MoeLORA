# Qwen3-VL-MoeLoRA

![](image/image.png)

> åœ¨åƒé—®æœ€æ–°çš„å¤šæ¨¡æ€ image-text æ¨¡å‹ **Qwen3-VL-4B-Instruct** ä¸Šå®Œæˆ MOELoRAï¼ˆæ··åˆä¸“å®¶ LoRAï¼‰å¾®è°ƒï¼ŒåŒæ—¶æ‰“é€š COCO 2014 æ•°æ®å¤„ç†ã€SwanLab ç›‘æ§ã€LangChain + RAG + Qt å¤šæ™ºèƒ½ä½“æ¨¡å‹éƒ¨ç½²çš„å…¨è¿‡ç¨‹ã€‚
> é¦–å…ˆç®€å•ä»‹ç»ä¸€ä¸‹ Qwen-vl å’Œ Qwen3-vl

<details>
<summary><strong>Qwen-VL</strong></summary>

> [Qwen3-VL Technical Report](https://arxiv.org/pdf/2511.21631)

å¦‚ä¸‹å›¾ï¼ŒQwen-VL ç³»åˆ— çš„è®­ç»ƒè¿‡ç¨‹åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼š

- **Stage1 ä¸ºé¢„è®­ç»ƒ**ï¼Œç›®æ ‡æ˜¯ä½¿ç”¨å¤§é‡çš„å›¾æ–‡ Pair å¯¹æ•°æ®å¯¹é½è§†è§‰æ¨¡å—å’Œ LLM çš„ç‰¹å¾ï¼Œè¿™ä¸ªé˜¶æ®µå†»ç»“ LLM æ¨¡å—çš„å‚æ•°ï¼›
- **Stage2 ä¸ºå¤šä»»åŠ¡é¢„è®­ç»ƒ**ï¼Œä½¿ç”¨æ›´é«˜è´¨é‡çš„å›¾æ–‡å¤šä»»åŠ¡æ•°æ®ï¼ˆä¸»è¦æ¥æºè‡ªå¼€æº VL ä»»åŠ¡ï¼Œéƒ¨åˆ†è‡ªå»ºæ•°æ®é›†ï¼‰ï¼Œæ›´é«˜çš„å›¾ç‰‡åƒç´ è¾“å…¥ï¼Œå…¨å‚æ•°è®­ç»ƒï¼›
- **Stage3 ä¸ºæŒ‡ä»¤å¾®è°ƒé˜¶æ®µ**ï¼Œè¿™ä¸ªé˜¶æ®µå†»ç»“è§†è§‰ Encoder æ¨¡å—ï¼Œä½¿ç”¨çš„æ•°æ®ä¸»è¦æ¥è‡ªå¤§æ¨¡å‹ Self-Instruction æ–¹å¼è‡ªåŠ¨ç”Ÿæˆï¼Œç›®æ ‡æ˜¯æå‡æ¨¡å‹çš„æŒ‡ä»¤éµå¾ªå’Œå¤šè½®å¯¹è¯èƒ½åŠ›ã€‚

![](image/Qwen-vl.png)
è€Œæœ€æ–°å¼€æºçš„ Qwen3-vl ä¸»è¦æœ‰å¦‚ä¸‹åˆ›æ–°ï¼š

- **Interleaved-MRoPEï¼š** åœ¨ æ—¶é—´/å®½/é«˜å¤šç»´åº¦åšå…¨é¢‘ç‡åˆ†é…çš„ä½ç½®ç¼–ç ï¼Œæå‡é•¿è§†é¢‘æ—¶åºæ¨ç†ã€‚
- **DeepStackï¼š** èåˆå¤šå±‚ ViT è§†è§‰ç‰¹å¾ï¼Œå¼ºåŒ–ç»†ç²’åº¦å¯¹é½ä¸è¯†åˆ«ã€‚
- **Textâ€“Timestamp Alignmentï¼š** ä» T-RoPE èµ°å‘â€œæ–‡æœ¬-æ—¶é—´æˆ³â€ç²¾å‡†å¯¹é½ï¼Œåˆ©äºäº‹ä»¶çº§è§†é¢‘å®šä½ã€‚
  ![](image/qwen3-vl.jpg)

</details>

---

## ğŸš€ é¡¹ç›®ä»‹ç»ä¸æ¨¡å—æ¦‚è§ˆ

1. **æ•°æ® â†’ è®­ç»ƒ â†’ æ¨ç†å…¨é“¾è·¯è„šæœ¬é½å…¨**ï¼š`download_data2csv.py` è´Ÿè´£æ‹‰å– & æ¸…æ´— COCO Captionï¼Œ`csv2json.py` é€‚é… Qwen3-VL æ ¼å¼ï¼Œ`MoeLORA.py/ lora.py` å¤„ç† LoRA / MoeLoRA è®­ç»ƒï¼Œ`test.py` å¿«é€ŸéªŒè¯ï¼Œ`multi_agent/` åˆ™æä¾› LangChain + RAG + Qt å¤šæ™ºèƒ½ä½“éƒ¨ç½²ã€‚
2. **æœ¬åœ°åŒ– RAG + å¤šæ¨¡æ€å¤šæ™ºèƒ½ä½“åŠ©æ‰‹**ï¼šPyQt5 æ¡Œé¢ç«¯ UIï¼Œæ•´åˆ LangChain æ£€ç´¢ã€FAISS å‘é‡åº“ä¸æœ¬åœ° Qwen3-VL æ¨ç†ï¼Œæ”¯æŒæ–‡æœ¬/å›¾åƒé—®ç­”ã€ä¸€é”®å¼€å…³çŸ¥è¯†åº“å¼•ç”¨ã€‚

- Advanced RAGï¼šBM25 + FAISS æ··åˆå¬å›ï¼Œç»“åˆ `BAAI/bge-reranker-base` Cross-Encoder é‡æ’åºï¼Œé»˜è®¤è¾“å‡ºæœ€ç›¸å…³çš„ Top-N ç‰‡æ®µã€‚
- Multi-Agent å‡çº§ï¼šæ–°å¢ Reviewer è‡ªæ£€æ­¥éª¤ï¼ˆæœ€å¤šé‡å†™ä¸€æ¬¡ç­”æ¡ˆï¼‰ä»¥åŠ MCP é£æ ¼ `save_session_summary` å·¥å…·ï¼Œå¯æŠŠèŠå¤©è®°å½•ä¸€é”®å¯¼å‡º Markdownã€‚

3. **è½»é‡æ˜¾å­˜å‹å¥½**ï¼šé»˜è®¤ 4-bit NF4 é‡åŒ– + LoRAï¼Œåªéœ€å•å¡ 8G ä¹Ÿèƒ½è·‘å®Œå¾®è°ƒæµç¨‹ã€‚
4. **SwanLab å…¨ç¨‹å¯è§†åŒ–**ï¼šè®­ç»ƒæ—¥å¿—ã€æŒ‡æ ‡å¯è§†åŒ–é½å¤‡ï¼Œä¾¿äºè°ƒä¼˜ä¸å¤ç°ã€‚

é¡¹ç›®æä¾› â€œæ•°æ®ä¸‹è½½ â†’ æ ¼å¼è½¬æ¢ â†’LoRA / MoeLoRA è®­ç»ƒ â†’ æœ¬åœ°æ¨ç† â†’ å¤šæ™ºèƒ½ä½“éƒ¨ç½²â€ çš„æœ€å°å¯å¤ç°å·¥ç¨‹ï¼Œå¸®åŠ©ä½ å¿«é€ŸéªŒè¯è‡ªå®šä¹‰çŸ¥è¯†åº“ + å¤šæ¨¡æ€é—®ç­”çš„å®Œæ•´é—­ç¯ã€‚

> ä»“åº“ä¸è‡ªå¸¦ Qwen3-VL-4B-Instruct æƒé‡ä¸ COCO æ•°æ®é›†ï¼Œè¯·æŒ‰ä¸‹æ–‡æŒ‡å¼•ä¸‹è½½åˆ°æŒ‡å®šç›®å½•ã€‚

## ğŸ“ é¡¹ç›®ç»“æ„

```text
Qwen3-VL-MoeLORA/
â”œâ”€â”€ coco_2014_caption/              # æ•°æ®é›†ä¸‹è½½ & è½¬æ¢äº§ç‰©
â”œâ”€â”€ multi_agent/                    # LangChain + RAG + Qt å¤šæ™ºèƒ½ä½“åŠ©æ‰‹
â”‚   â”œâ”€â”€ main_app.py                 # PyQt5 ä¸»ç¨‹åº
â”‚   â”œâ”€â”€ langchain_wrappers.py       # æœ¬åœ° Qwen æ–‡æœ¬/å¤šæ¨¡æ€å°è£…
â”‚   â”œâ”€â”€ multi_agent.py              # Planner / Manager / Knowledge / Reviewer / Responder ç¼–æ’
â”‚   â”œâ”€â”€ model_client.py             # æ¨¡å‹åŠ è½½ä¸å›¾ç‰‡è¾“å…¥å¤„ç†
â”‚   â”œâ”€â”€ rag_pipeline.py             # çŸ¥è¯†åº“åŠ è½½ + æ··åˆæ£€ç´¢ + äº¤å‰ç¼–ç é‡æ’åº
â”‚   â”œâ”€â”€ requirements.txt            # å­æ¨¡å—ä¾èµ–ï¼ˆPyQt5ã€LangChainã€FAISS ç­‰ï¼‰
â”‚   â”œâ”€â”€ tools.py                    # MCP é£æ ¼å·¥å…·ï¼ˆsave_session_summary ç­‰ï¼‰
â”‚   â””â”€â”€ knowledge_base/             # é»˜è®¤çŸ¥è¯†åº“ï¼Œå¯æ”¾ txt/md/pdf
â”œâ”€â”€ download_model.py               # ä¸‹è½½ Qwen3-VL åŸºåº§åˆ°æœ¬åœ°
â”œâ”€â”€ download_data2csv.py            # æ‹‰å– ModelScope æ•°æ®é›†
â”œâ”€â”€ csv2json.py                     # æ•°æ®è½¬æ¢è„šæœ¬
â”œâ”€â”€ MoeLORA.py / lora.py            # LoRA / MoeLoRA è®­ç»ƒ
â”œâ”€â”€ test.py                         # æœ¬åœ°æ¨ç† DEMO
â”œâ”€â”€ main_app_ui.py / main_langchain_ui.py # Qt UI ç¤ºä¾‹
â”œâ”€â”€ output/                         # LoRA è®­ç»ƒç»“æœ
â”œâ”€â”€ qwen3-vl-4b-instruct/           # é¢„æœŸæ”¾ç½®å®˜æ–¹åŸºåº§
â””â”€â”€ requirements.txt                # é¡¶å±‚ä¾èµ–
```

## âš™ï¸ å¿«é€Ÿä¸Šæ‰‹

### 1. å…‹éš† & åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ

```powershell
git clone https://github.com/<your-account>/Qwen3-VL-MoeLORA.git
cd Qwen3-VL-MoeLORA
python -m venv .venv
.venv\Scripts\activate
pip install -r requirements.txt
```

> å¤šæ™ºèƒ½ä½“æ¡Œé¢ç«¯ä¾èµ–é¢å¤–çš„ PyQt5/FAISSï¼Œå¯åœ¨ `multi_agent/` ç›®å½•æ‰§è¡Œ `pip install -r requirements.txt`ã€‚

### 2. ä¸‹è½½æ¨¡å‹ä¸æ•°æ®é›†

```powershell
# æ‹‰å–å®˜æ–¹åŸºåº§
python download_model.py --target ./qwen3-vl-4b-instruct

# ä¸‹è½½ COCO Caption ç¤ºä¾‹å¹¶å†™å…¥ CSV
python download_data2csv.py --output ./coco_2014_caption/train.csv

# è½¬æ¢ä¸º Qwen3-VL JSONï¼ˆå¯æŒ‡å®šæ¡ç›®ï¼‰
python csv2json.py --csv ./coco_2014_caption/train.csv --json ./coco_2014_caption/train.json --top_k 500
```

### 3. å¿«é€Ÿæ¨ç†ï¼ˆåŸºåº§æˆ– LoRAï¼‰

```powershell
python test.py --model ./qwen3-vl-4b-instruct --image ./image/demo.jpg --prompt "æè¿°è¿™å¼ å›¾ç‰‡"
```

è‹¥å·²å®Œæˆ LoRA è®­ç»ƒï¼Œå¯å°† `--model` æŒ‡å‘åˆå¹¶åçš„æƒé‡æˆ–ç›´æ¥åœ¨ `test.py` ä¸­åŠ è½½ `PeftModel`ã€‚

### 4. å¯åŠ¨ LangChain + RAG + Qt å¤šæ™ºèƒ½ä½“åŠ©æ‰‹

```powershell
cd multi_agent
pip install -r requirements.txt  # é¦–æ¬¡æ‰§è¡Œ
python main_app.py
```

å·¦ä¾§è¾“å…¥é—®é¢˜/ä¸Šä¼ å›¾ç‰‡ï¼Œå³ä¾§ä¼šå±•ç¤º Planner/Manager è®¡åˆ’ã€RAG æ£€ç´¢æ‘˜è¦ä¸æœ€ç»ˆå›å¤ã€‚

#### ğŸŒŸ æ–°ç‰ˆå¤šæ™ºèƒ½ä½“äº®ç‚¹

- **Advanced RAG**ï¼šBM25 + FAISS èåˆå¬å›ï¼Œå åŠ  Cross-Encoder é‡æ’åºï¼Œæ˜¾è‘—é™ä½â€œæŸ¥ä¸åˆ°/æŸ¥ä¸å‡†â€é—®é¢˜ã€‚
- **Reviewer QA Loop**ï¼šResponder ç”Ÿæˆç­”æ¡ˆåï¼ŒReviewer ä»¥ PASS/RETRY å½¢å¼å¤æ ¸ï¼›å¦‚ä¸åˆæ ¼ï¼Œä¼šæŠŠå»ºè®®åé¦ˆç»™ Responder é‡å†™ä¸€æ¬¡ï¼Œæå‡å›ç­”å®Œæ•´æ€§ã€‚
- **MCP é£æ ¼å½’æ¡£å·¥å…·**ï¼šå†…ç½® `save_session_summary`ï¼Œå½“èŠå¤©è¯­å¥åŒ…å«â€œæ€»ç»“å¯¹è¯ / ä¿å­˜è®°å½• / archive / summary â€¦â€æ—¶ï¼Œä¼šæŠŠå®Œæ•´å¯¹è¯ã€è°ƒåº¦è®¡åˆ’ä¸æ£€ç´¢è¯æ®å†™å…¥ `multi_agent/output/reports/*.md`ï¼Œå›ç­”é‡Œä¹Ÿä¼šæ˜¾ç¤ºæ–‡ä»¶è·¯å¾„ã€‚
- **GUI è‡ªåŠ¨è®°å½•ä¸Šä¸‹æ–‡**ï¼šå‰ç«¯æŒç»­è·Ÿè¸ªæ¯è½®é—®ç­”ï¼Œç”Ÿæˆæ€»ç»“æ—¶æ— éœ€æ‰‹å·¥å¤åˆ¶å†…å®¹ï¼ŒAgent ä¼šè‡ªåŠ¨è¯»å–å†å²å¯¹è¯ã€‚

> å¯ä»¥çœ‹åˆ°æ™ºèƒ½ä½“æ­£ç¡®å›ç­”æˆ‘çš„é—®é¢˜ï¼šå°è‹”è—“äºä»Šå¹´ 9 æœˆ 25 æ—¥ä¿é€åˆ°å¦é—¨å¤§å­¦ï¼ˆä¹Ÿå°±æ˜¯æœ¬äººï¼Œå“ˆå“ˆï¼Œç›®å‰ GUI è¿˜æœ‰ç‚¹å°é—®é¢˜ï¼Œæ–‡å­—å†…å®¹ä¸é•¿å±•ç¤ºä¸å…¨ï¼‰

![](image/image1.png)

> å¯¼å‡ºä¼šè¯è®°å½•
> ![](image/images3.png)

### 5. LoRA / MoeLoRA å¾®è°ƒ

```powershell
python MoeLORA.py \
  --model ./qwen3-vl-4b-instruct \
  --train_json ./coco_2014_caption/train.json \
  --output_dir ./output/Qwen3-VL-4Blora
```

è„šæœ¬é»˜è®¤å¯ç”¨ BitsAndBytes 4-bit ä¸ PEFTï¼Œå¯æ ¹æ®æ˜¾å­˜æƒ…å†µè°ƒæ•´ `r`ã€`lora_alpha`ã€`gradient_accumulation_steps` ç­‰å‚æ•°ã€‚è®­ç»ƒå®Œæˆåäº§ç‰©ä½äº `output/`ï¼Œå¯è¢«å¤šæ™ºèƒ½ä½“æˆ– `test.py` ç›´æ¥åŠ è½½ã€‚

##### ä»£ç åŒ…å«å®Œæ•´æµç¨‹ä»£ç 

ï¼ˆæ³¨ï¼šä¸åŒ…å« Qwen3-VL-4B-Instruct æ¨¡å‹ä»£ç å’Œæƒé‡ï¼Œè¯·è‡ªè¡Œä¸‹è½½ï¼‰ï¼š
coco_2014_caption æ•°æ®é›† [coco_2014_caption](https://modelscope.cn/datasets/modelscope/coco_2014_caption/quickstart)
Qwen3-VL-4B-Instruct æ¨¡å‹ [Qwen3-VL-4B-Instruct](https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct)
ä¸‹è½½åç›´æ¥æ”¾åœ¨é¡¹ç›®æ‰€åœ¨ç›®å½•å³å¯ï¼ˆ`./qwen3-vl-4b-instruct`ï¼‰

ModelScope æ•°æ®é›†åŠ è½½->å¤šæ¨¡æ€æ•°æ®é¢„å¤„ç†->lora\MOELora å¾®è°ƒé…ç½®->SwanLab è®­ç»ƒå¯è§†åŒ–åŠå¾®è°ƒåæ¨¡å‹æ¨ç†

## ğŸ§  LangChain + RAG + Qt å¤šæ™ºèƒ½ä½“åŠ©æ‰‹

- **Planner Agent**ï¼šæ‹†è§£ä»»åŠ¡ï¼Œåˆ—å‡º 3-5 ä¸ªå…³é”®æ­¥éª¤å¹¶æç¤ºæ˜¯å¦éœ€è¦çŸ¥è¯†åº“ã€‚
- **Manager Agent**ï¼šç»“åˆ Planner è¾“å‡ºä¸ RAG é¢„è§ˆï¼Œå†³å®šæ˜¯å¦ç»§ç»­æ£€ç´¢ã€ç»™å‡ºé¢å¤–æç¤ºã€‚
- **Knowledge Agent**ï¼šåŸºäº `rag_pipeline.py` æ„å»ºçš„ BM25 + FAISS æ··åˆæ£€ç´¢ + Cross-Encoder é‡æ’åºï¼Œæ”¯æŒ `.txt/.md/.pdf` å¤šç¼–ç åŠ è½½ã€‚
- **Responder Agent**ï¼šé€šè¿‡ `model_client.py` è°ƒç”¨æœ¬åœ° Qwen3-VLï¼ˆæ”¯æŒ LoRA æƒé‡ & å›¾æ–‡è¾“å…¥ï¼‰ï¼Œå¹¶èƒ½æ¥æ”¶ Reviewer çš„æ”¹å†™å»ºè®®ã€‚
- **Reviewer Agent**ï¼šæ£€æŸ¥ç­”æ¡ˆæ˜¯å¦è§£å†³ç”¨æˆ·é—®é¢˜ï¼Œå¿…è¦æ—¶è¦æ±‚ Responder é‡ç­”ã€‚
- **Qt å‰ç«¯**ï¼š`main_app.py` å¤åˆ» `main_langchain_ui.py` çš„äº¤äº’ä½“éªŒï¼Œå±•ç¤º Planner/Manager é¢æ¿ã€RAG å‚è€ƒä¸æœ€ç»ˆç­”å¤

> é»˜è®¤çŸ¥è¯†åº“ç›®å½•ï¼š`multi_agent/knowledge_base/`ï¼Œç•Œé¢å³ä¸‹è§’å¯å‹¾é€‰ â€œå¯ç”¨çŸ¥è¯†åº“æ£€ç´¢ (RAG)â€ å¼€å…³ã€‚è‹¥åªéœ€å½’æ¡£èŠå¤©ï¼Œè¯·è¾“å…¥â€œæ€»ç»“å¯¹è¯/ä¿å­˜è®°å½•â€ç­‰æŒ‡ä»¤ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨è°ƒç”¨ MCP å·¥å…·è¾“å‡º Markdownã€‚

##### ä¸Šä¼ æœ¬åœ°è®ºæ–‡åˆ°çŸ¥è¯†åº“ï¼Œå¯¹è®ºæ–‡è¿›è¡Œé—®ç­”

![](image/image0.png)

#### 1.æœ¬åœ°éƒ¨ç½²æ¨ç†

æ¨¡å‹ä» huggingface ä¸‹è½½åˆ°æœ¬åœ°åï¼Œå°† test.py ä¸­çš„ model_id æ¢ä¸ºæœ¬åœ°è·¯å¾„ï¼Œè¿è¡Œ test.py æ–‡ä»¶

![](image/image-20251018215438537.png)

#### 2.å¾®è°ƒ

lora é…ç½®ï¼Œè§ MoeLORA.py æ–‡ä»¶

```python
config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    # æ–°å¢è§†è§‰ç¼–ç å™¨å’Œäº¤å‰æ³¨æ„åŠ›å±‚ï¼ˆQwen3-VLç‰¹æœ‰æ¨¡å—ï¼‰
    target_modules=[
        # æ–‡æœ¬æ¨¡å—
        "q_proj", "k_proj", "v_proj", "o_proj"
        # è§†è§‰æ¨¡å—
        "visual_q_proj", "visual_k_proj"],
    inference_mode=False,
    r=8,  # 8Gæ˜¾å­˜å»ºè®®r=16ï¼ˆåŸ64å¯èƒ½æ˜¾å­˜ä¸è¶³ï¼‰
    lora_alpha=16,
    lora_dropout=0.05,
    bias="none",
)
```

![](image/image-20251019172207276.png)

å¾®è°ƒå›¾åƒ

![](image/image-20251019210718609.png)

<details>
<summary><strong>è®­ç»ƒè¿è¡Œï¼ˆlogï¼‰</strong></summary>
| æŒ‡æ ‡                          | LoRA                                                        | MoeLoRA                                                     | Adapter (IA3)                                               |
| ----------------------------- | ----------------------------------------------------------- | ----------------------------------------------------------- | ----------------------------------------------------------- |
| æ¨¡å‹ï¼ˆbaseï¼‰                  | `qwen3-vl-4b-instruct`ï¼ˆæœ¬åœ°è·¯å¾„ `./qwen3-vl-4b-instruct`ï¼‰ | `qwen3-vl-4b-instruct`ï¼ˆæœ¬åœ°è·¯å¾„ `./qwen3-vl-4b-instruct`ï¼‰ | `qwen3-vl-4b-instruct`ï¼ˆæœ¬åœ°è·¯å¾„ `./qwen3-vl-4b-instruct`ï¼‰ |
| æ•°æ®é›†æ ·æœ¬æ•°ï¼ˆtrainï¼‰         | 496 examples                                                | 496 examples                                                | 496 examples                                                |
| æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆè„šæœ¬ï¼‰        | 8192 tokens                                                 | 8192 tokens                                                 | 8192 tokens                                                 |
| å¾®è°ƒæ–¹æ³•                      | LoRA (PEFT) + 4-bit quantization (bnb nf4)                  | MoeLoRA (å¤šä¸“å®¶ LoRA) + 4-bit quantization (bnb nf4)        | IA3 (Adapter) + 4-bit quantization (bnb nf4)                |
| æ³¨å…¥çš„å¯è®­ç»ƒå‚æ•°ï¼ˆæ—¥å¿—ï¼‰      | 5,898,240 trainable params                                  | 10,298,240 trainable params                                 | 7,340,928 trainable paramsï¼ˆâ‰ˆ734 ä¸‡ï¼‰                       |
| æ¨¡å‹æ€»å‚æ•°é‡ï¼ˆæ—¥å¿—ï¼‰          | 4,443,714,048 å…¨é‡å‚æ•°                                      | 4,443,714,048 å…¨é‡å‚æ•°                                      | 4,443,714,048 å…¨é‡å‚æ•°                                      |
| trainable ç™¾åˆ†æ¯”ï¼ˆæ—¥å¿—ï¼‰      | ~0.1327%                                                    | â‰ˆ0.245%                                                     | â‰ˆ0.165%                                                     |
| è®­ç»ƒè½®æ¬¡ (epochs)             | 5.0 epochs                                                  | 5.0 epochs                                                  | 5.0 epochs                                                  |
| æ€»è®­ç»ƒæ­¥æ•°ï¼ˆglobal stepsï¼‰    | 310 steps                                                   | 310 steps                                                   | 310 steps                                                   |
| æ¯ epoch æ­¥æ•°                 | ~62 steps/epoch                                             | ~62 steps/epoch                                             | ~62 steps/epoch                                             |
| per_device_train_batch_size   | 1                                                           | 1                                                           | 1                                                           |
| gradient_accumulation_steps   | 8                                                           | 8                                                           | 8                                                           |
| å­¦ä¹ ç‡ï¼ˆåˆå§‹ï¼‰                | 1e-4                                                        | 1e-4                                                        | 1e-4                                                        |
| å­¦ä¹ ç‡ï¼ˆè®­ç»ƒæœ«æœŸï¼‰            | 3.2258e-07                                                  | 3.2258e-07                                                  | 3.2258e-07                                                  |
| è®­ç»ƒæ€»æ—¶é•¿                    | 5666.18 s â‰ˆ 94.44 min                                       | 7802.80 s â‰ˆ 130.05 min                                      | 5347.34 s â‰ˆ 89.12 min                                       |
| å¹³å‡ train_lossï¼ˆå…¨ç¨‹ï¼‰       | 1.5232                                                      | 1.4217                                                      | 1.9296                                                      |
| åˆå§‹ batch lossï¼ˆç¬¬ä¸€æ¡æ—¥å¿—ï¼‰ | 4.8942                                                      | 4.8942                                                      | 4.894                                                       |
| è®­ç»ƒæ ·æœ¬åå                  | 0.438 samples/s                                             | 0.318 samples/s                                             | 0.464 samples/s                                             |
| è®­ç»ƒæ­¥åå                    | 0.055 steps/s                                               | 0.04 steps/s                                                | 0.058 steps/s                                               |
| æ¢¯åº¦èŒƒæ•°ï¼ˆè§‚æµ‹èŒƒå›´ï¼‰          | çº¦ 1.65 â€” 3.82                                              | çº¦ 1.57 â€” 4.06                                              | çº¦ 0.57 â€” 2.83                                              |
| é‡åŒ–æ–¹å¼                      | 4-bit NF4 åŒé‡åŒ–ï¼Œcompute_dtype=float16                     | 4-bit NF4 åŒé‡åŒ–ï¼Œcompute_dtype=float16                     | 4-bit NF4 åŒé‡åŒ–ï¼Œcompute_dtype=float16                     |
| mixed-precision               | fp16=True                                                   | fp16=True                                                   | fp16=True                                                   |

åŠ è½½è®­ç»ƒå¥½çš„ LoRA checkpoint åšæ¨ç†

```python
from peft import PeftModel
from transformers import AutoModelForImageTextToText

base = AutoModelForImageTextToText.from_pretrained(model_id,
                                                  quantization_config=bnb_config,
                                                  device_map={"": "cuda"},
                                                  trust_remote_code=True)
base.config.use_cache = False
infer_model = PeftModel.from_pretrained(base, "./output/Qwen3-VL-4Blora")  # æœ¬åœ°è·¯å¾„
infer_model.to("cuda").eval()

```

</details>

---

æ³¨æ„ï¼šä¸è¦æŠŠæœ¬åœ°è·¯å¾„ä»¥ `model_id=` å½¢å¼ä¼ ç»™ `from_pretrained` é‡Œä¼šè§¦å‘ HF repo id éªŒè¯ï¼ˆæ—¥å¿—é‡Œå·²è§é”™è¯¯æç¤ºï¼‰ã€‚ç›´æ¥æŠŠæœ¬åœ° checkpoint ç›®å½•è·¯å¾„ä½œä¸ºç¬¬ä¸€ä¸ªå‚æ•°ä¼ å…¥ `PeftModel.from_pretrained` å³å¯ã€‚

å¾®è°ƒåæ¨ç†ç»“æœ
![](image/PixPin_2025-11-03_17-12-57.png)

## ğŸ§­ é¡¹ç›®è§„åˆ’ï¼ˆRoadmapï¼‰

- [ ] **MCP (Model Context Protocol)**ï¼šå°†å¤šæ™ºèƒ½ä½“æ¨ç†å°è£…æˆ MCP Serverï¼Œæ–¹ä¾¿ IDE / Copilot Chat ç›´æ¥è°ƒç”¨ã€‚
- [ ] **çŸ¥è¯†åº“çƒ­æ›´æ–°**ï¼šæ”¯æŒè¿œç¨‹å‘é‡æ•°æ®åº“ï¼ˆMilvus/Elasticsearchï¼‰ä¸åœ¨çº¿æ–‡æ¡£è‡ªåŠ¨åŒæ­¥ã€‚
- [ ] **è¯„æµ‹è‡ªåŠ¨åŒ–**ï¼šå¼•å…¥ LLM-as-a-judgeã€BLEU/ROUGE ç­‰æŒ‡æ ‡ï¼Œå¯¹å›¾æ–‡å›ç­”åšè‡ªåŠ¨éªŒæ”¶ã€‚
- [ ] **æ•°æ®æ‰©å±•æµæ°´çº¿**ï¼šæŠ“å–è¡Œä¸šæŠ¥å‘Š â†’`csv2json.py` è‡ªåŠ¨è½¬æ¢ â†’ ä¸€é”®åŠ å…¥è®­ç»ƒæˆ– RAGã€‚

###### è‡´è°¢:

[Qwen3-VL](https://github.com/QwenLM/Qwen3-VL)
